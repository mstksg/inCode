<!DOCTYPE HTML>
<html><head><title>A Purely Functional Typed Approach to Trainable Models (Part 3) · in Code</title><meta name="description" content="Weblog of Justin Le, covering various adventures in programming and explorations in the worlds of computation physics, and knowledge.
"><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta name="flattr:id" content="3p9jqr"><meta property="og:site_name" content="in Code"><meta property="og:description" content="Hi again! Today we’re going to jump straight into tying together the functional framework described in this series and see how it can give us some interesting insight, as well as wrapping it up by talking about the scaffolding needed to turn this all into a working system you can apply today. The name of the game is a purely functional typed approach to writing trainable models using differentiable programming. Be sure to check out Part 1 and Part 2 if you haven’t, because this is a direct continuation. My favorite part about this system really is how we have pretty much free reign over how we can combine and manipulate our models, since they are just functions. Combinators — a word I’m going to be using to mean higher-order functions that return functions — tie everything together so well. Some models we might have thought were standalone entities might just be derivable from other models using basic functional combinators. And the best part is that they’re never necessary; just helpful. Again, if you want to follow along, the source code for the written code in this module is available on github."><meta property="og:type" content="article"><meta property="og:title" content="A Purely Functional Typed Approach to Trainable Models (Part 3)"><meta property="og:image" content="https://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://blog.jle.im/entry/purely-functional-typed-models-3.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="https://blog.jle.im/entry/purely-functional-typed-models-3.html"><link href="https://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="https://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="https://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="https://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="https://blog.jle.im/js/social.js"></script><script type="text/javascript" src="https://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="https://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="https://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="https://blog.jle.im/">home</a></li><li><a href="https://blog.jle.im/entries.html">archives</a></li><li><a href="https://cv.jle.im">cv</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><h1 id="title">A Purely Functional Typed Approach to Trainable Models (Part 3)</h1><p class="entry-info">by <a class="author" href="https://blog.jle.im/">Justin Le</a><span class="info-separator"> &diams; </span><time datetime="2018-05-14T12:16:34Z" pubdate="" class="pubdate">Monday May 14, 2018</time></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/functional-models-3.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://github.com/mstksg/inCode/tree/gh-pages/entry/purely-functional-typed-models-3.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/purely-functional-typed-models-3.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled...really just the king of great languages.
">Haskell</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>Hi again! Today we’re going to jump straight into tying together the functional framework described in this series and see how it can give us some interesting insight, as well as wrapping it up by talking about the scaffolding needed to turn this all into a working system you can apply today.</p>
<p>The name of the game is a purely functional typed approach to writing trainable models using differentiable programming. Be sure to check out <a href="https://blog.jle.im/entry/purely-functional-typed-models-1.html">Part 1</a> and <a href="https://blog.jle.im/entry/purely-functional-typed-models-2.html">Part 2</a> if you haven’t, because this is a direct continuation.</p>
<p>My favorite part about this system really is how we have pretty much free reign over how we can combine and manipulate our models, since they are just functions. Combinators — a word I’m going to be using to mean higher-order functions that return functions — tie everything together so well. Some models we might have thought were standalone entities might just be derivable from other models using basic functional combinators. And the best part is that they’re never <em>necessary</em>; just <em>helpful</em>.</p>
<p>Again, if you want to follow along, the source code for the written code in this module is available <a href="https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs">on github</a>.</p>
<h2 id="combinator-fun">Combinator Fun</h2>
<h3 id="recurrence">Recurrence</h3>
<p>Here’s one example of how the freedom that “normal functions” gives you can help reveal insight. While working through this approach, I stumbled upon an interesting way of defining recurrent neural networks — a lot of times, a “recurrent neural network” really just means that some function of the <em>previous</em> output is used as an “extra input”.</p>
<p>This sounds like we can really write a recurrent model as a “normal” model, and then use a combinator to feed it back into itself.</p>
<p>To say in types:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>recurrently</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> <span class="dt">Model</span>  p   (a <span class="op">:&amp;</span> b) b</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p b  a       b</span></code></pre></div>
<p>A “normal, non-stateful model” taking an <code>a :&amp; b</code> and returning a <code>b</code> can really be turned into a stateful model with state <code>b</code> (the <em>previous output</em>) and only taking in an <code>a</code> input.</p>
<p>This sort of combinator is a joy to write in Haskell because it’s a “follow the types” kinda deal — you set up the function, and the compiler pretty much writes it for you, because the types guide the entire implementation:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L305-L311</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>recurrently</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> b)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span>  p   (a <span class="op">:&amp;</span> b) b</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p b  a       b</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>recurrently f p x yLast <span class="ot">=</span> (y, y)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> f p (x <span class="op">:&amp;&amp;</span> yLast)</span></code></pre></div>
<p>In general though, it’d be nice to have <em>some function</em> of the previous output be stored as the state. We can write this combinator as well, taking the function that transforms the previous output into the stored state:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L313-L320</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>recurrentlyWith</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> b)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> (<span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span> <span class="ot">=&gt;</span> <span class="dt">BVar</span> z c <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Model</span>  p   (a <span class="op">:&amp;</span> b) c</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p b  a       c</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>recurrentlyWith store f p x yLast <span class="ot">=</span> (y, store y)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> f p (x <span class="op">:&amp;&amp;</span> yLast)</span></code></pre></div>
<p>Again, once we figure out the <em>type</em> our combinator has…the function <em>writes itself</em>. The joys of Haskell! I wouldn’t dare try to write this in a language without static types and type inference. But it’s a real treat to write this out in a language like Haskell.</p>
<p><code>recurrentlyWith</code> takes a <code>c -&gt; b</code> function and turns a pure model taking an <code>a :&amp; b</code> into a stateful model with state <code>b</code> taking in an <code>a</code>. The <code>c -&gt; b</code> tells you how to turn the previous output into the new state.</p>
<p>To me, <code>recurrentlyWith</code> captures the “essence” of what a recurrent model or recurrent neural network is — the network is allowed to “see” some form of its previous output.</p>
<p>How is this useful? Well, we can use this to define a fully connected recurrent neural network layer as simply a recurrent version of a normal fully connected feed-forward layer.</p>
<p>We can redefine a pre-mapped version of <code>feedForward</code> which takes a tuple of two vectors and concatenates them before doing anything:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- | Concatenate two vectors</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="ot">(#) ::</span> <span class="dt">BVar</span> z (<span class="dt">R</span> i) <span class="ot">-&gt;</span> <span class="dt">BVar</span> z (<span class="dt">R</span> o) <span class="ot">-&gt;</span> <span class="dt">BVar</span> z (<span class="dt">R</span> (i <span class="op">+</span> o))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L322-L325</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>ffOnSplit</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> <span class="kw">forall</span> i o<span class="op">.</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> _ (<span class="dt">R</span> i <span class="op">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> o)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>ffOnSplit p (rI <span class="op">:&amp;&amp;</span> rO) <span class="ot">=</span> feedForward p (rI <span class="op">#</span> rO)</span></code></pre></div>
<p><code>ffOnSplit</code> is a feed-forward layer taking an <code>R (i + o)</code>, except we pre-map it to take a tuple <code>R i :&amp; R o</code> instead. This isn’t anything special, just some plumbing.</p>
<p>Now our fully connected recurrent layer is just <code>recurrentlyWith logistic ffOnSplit</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>fcrnn&#39;</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> _ (<span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>fcrnn&#39; <span class="ot">=</span> recurrentlyWith logistic ffOnSplit</span></code></pre></div>
<p>Basically just a recurrent version of <code>feedForward</code>! If we factor out some of the manual uncurrying and pre-mapping, we get a nice functional definition:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L327-L330</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>fcrnn&#39;</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">ModelS</span> _ (<span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>fcrnn&#39; <span class="ot">=</span> recurrentlyWith logistic (\p <span class="ot">-&gt;</span> feedForward p <span class="op">.</span> uncurryT (<span class="op">#</span>))</span></code></pre></div>
<h3 id="lag">Lag</h3>
<p>Another interesting result – we can write a “lagged” combinator that takes a model expecting a vector as an input, and turn it into a stateful model taking a <em>single</em> input, and feeding the original model that input and also a history of the <code>n</code> most recent inputs.</p>
<p>If that sounds confusing, let’s just try to state it out using types:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ot">lagged ::</span> <span class="dt">Model</span>  p       (<span class="dt">R</span> (n <span class="op">+</span> <span class="dv">1</span>)) b</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>       <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p (<span class="dt">R</span> n) <span class="dt">Double</span>      b</span></code></pre></div>
<p>The result is a <code>ModelS p (R n) Double b</code>; the state is the <code>n</code> most recent inputs, and it feeds that in at every step and keeps it updated. Let’s write it using <code>headTail</code> and <code>&amp;</code>, which splits a vector and adds an item to the end, respectively.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L332-L340</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>lagged</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> n, <span class="dv">1</span> <span class="op">&lt;=</span> n)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span>  p       (<span class="dt">R</span> (n <span class="op">+</span> <span class="dv">1</span>)) b</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">ModelS</span> p (<span class="dt">R</span> n) <span class="dt">Double</span>      b</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>lagged f p x xLasts <span class="ot">=</span> (y, xLasts&#39;)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    fullLasts    <span class="ot">=</span> xLasts <span class="op">&amp;</span> x</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    y            <span class="ot">=</span> f p fullLasts</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    (_, xLasts&#39;) <span class="ot">=</span> headTail fullLasts</span></code></pre></div>
<p>What can we do with this? Well… we can write a general autoregressive model AR(p) of <em>any</em> degree, simply by lagging a fully connected ANN layer:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L342-L344</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="ot">ar ::</span> (<span class="dt">KnownNat</span> n, <span class="dv">1</span> <span class="op">&lt;=</span> n)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>   <span class="ot">=&gt;</span> <span class="dt">ModelS</span> _ (<span class="dt">R</span> n) <span class="dt">Double</span> <span class="dt">Double</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>ar <span class="ot">=</span> lagged (\p <span class="ot">-&gt;</span> <span class="fu">fst</span> <span class="op">.</span> headTail <span class="op">.</span> feedForward <span class="op">@</span>_ <span class="op">@</span><span class="dv">1</span> p)</span></code></pre></div>
<p>(using <code>fst . headTail</code> to extract the first <code>Double</code> from an <code>R 1</code>)</p>
<p>And that’s it! Our original AR(2) <code>ar2</code> is just <code>ar @2</code> … and we can write can write an AR(10) model by just using <code>ar @10</code>, and AR(20) model with <code>ar @20</code>, etc.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L346-L347</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="ot">ar2&#39; ::</span> <span class="dt">ModelS</span> _ (<span class="dt">R</span> <span class="dv">2</span>) <span class="dt">Double</span> <span class="dt">Double</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>ar2&#39; <span class="ot">=</span> ar <span class="op">@</span><span class="dv">2</span></span></code></pre></div>
<p>Who would have thought that an autoregressive model is just a fully connected neural network layer with lag?</p>
<p>Take a fully connected ANN layer and add recurrence — you get a fully connected RNN layer. Take a fully connected ANN layer and add lag — you get an autoregressive model from statistics!</p>
<p>There are many more such combinators possible! Combinators like <code>recurrentlyWith</code> and <code>lagged</code> just scratch the surface. Best of all, they help reveal to us that seemingly exotic things really are just simple applications of combinators from other basic things.</p>
<h2 id="fun-with-explicit-types">Fun with explicit types</h2>
<p>One of the advantages of the statically typed functional approach is that it forces you to keep track of parameter types as a part of your model manipulation. You can explicitly keep track of them, or let the compiler do it for you (and have the information ready when you need it). In what we have been doing so far, we have been letting the compiler have the fun. But we can get some interesting results with explicit manipulation of types, as well.</p>
<p>For example, an <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoder</a> is a type of model that composes a function that “compresses” information with a function that “decompresses” it; training an autoencoder involves training the composition of those two functions to produce the identity function.</p>
<p>We can represent a simple autoencoder:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ot">encoder ::</span> <span class="dt">Model</span> q (<span class="dt">R</span> <span class="dv">100</span>) (<span class="dt">R</span> <span class="dv">5</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="ot">decoder ::</span> <span class="dt">Model</span> p (<span class="dt">R</span> <span class="dv">5</span>)   (<span class="dt">R</span> <span class="dv">100</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ot">autoencoder ::</span> <span class="dt">Model</span> (p <span class="op">:&amp;</span> q) (<span class="dt">R</span> <span class="dv">100</span>) (<span class="dt">R</span> <span class="dv">100</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>autoencoder <span class="ot">=</span> decoder <span class="op">&lt;~</span> encoder</span></code></pre></div>
<p><code>autoencoder</code> now “encodes” a 100-dimensional space into a 5-dimensional one.</p>
<p>We can train <code>autoencoder</code> on our data set, but keep the “trained parameters” separate:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> decParam <span class="op">:&amp;</span> encParam <span class="ot">&lt;-</span> trainModelIO autoencoder <span class="op">$</span> <span class="fu">map</span> (\x <span class="ot">-&gt;</span> (x,x)) samps</span></code></pre></div>
<p>Now <code>decParam</code> and <code>encParam</code> make <code>autoencoder</code> an identity function. But, we can just use <code>encParam</code> with <code>encoder</code> to <em>encode</em> data, and <code>decParam</code> with <code>decoder</code> to <em>decode</em> data!</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>evalBP2 encoder<span class="ot"> encParam ::</span> <span class="dt">R</span> <span class="dv">100</span> <span class="ot">-&gt;</span> <span class="dt">R</span> <span class="dv">5</span>        <span class="co">-- trained encoder</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>evalBP2 decoder<span class="ot"> decParam ::</span> <span class="dt">R</span> <span class="dv">5</span>   <span class="ot">-&gt;</span> <span class="dt">R</span> <span class="dv">100</span>      <span class="co">-- trained decoder</span></span></code></pre></div>
<p>The types help by keeping track of what goes with what, so you don’t have to; the compiler helps you match up <code>encoder</code> with <code>encParam</code>, and can even “fill in the code” for you if you leave in a typed hole!</p>
<h2 id="a-unified-representation">A Unified Representation</h2>
<p>This section now is a small aside for those familiar with more advanced Haskell techniques like DataKinds and dependent types; if you aren’t too comfortable with these, feel free to skip to the next section! This stuff won’t come up again later.</p>
<p>If you’re still reading, one ugly thing you might have noticed was that we had to give different “types” for both our <code>Model</code> and <code>ModelS</code>, so we cannot re-use useful functions on both. For example, <code>mapS</code> only works on <code>ModelS</code>, but not <code>Model</code>. <code>(&lt;~)</code> only works on <code>Model</code>s, <code>(&lt;*~*)</code> only works on two <code>ModelS</code>s, and we had to define a different combinator <code>(&lt;*~)</code>.</p>
<p>This is not a fundamental limitation! With <em>DataKinds</em> and dependent types we can unify these both under a common type. If we had:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Model</span> (<span class="ot">p ::</span> <span class="dt">Type</span>) (<span class="ot">a ::</span> <span class="dt">Type</span>) (<span class="ot">b ::</span> <span class="dt">Type</span>) <span class="ot">=</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>       <span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">BVar</span> z p</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">ModelS</span> (<span class="ot">p ::</span> <span class="dt">Type</span>) (<span class="ot">s ::</span> <span class="dt">Type</span>) (<span class="ot">a ::</span> <span class="dt">Type</span>) (<span class="ot">b ::</span> <span class="dt">Type</span>) <span class="ot">=</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>       <span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">BVar</span> z p</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z s</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> (<span class="dt">BVar</span> z b, <span class="dt">BVar</span> z s)</span></code></pre></div>
<p>We can unify them by making either the <code>p</code> or <code>s</code> be optional, a <code>Maybe Type</code>, and using the <code>Option</code> type from <em><a href="https://hackage.haskell.org/package/type-combinators/docs/Data-Type-Option.html">Data.Type.Option</a></em>, from the <em><a href="https://hackage.haskell.org/package/type-combinators">type-combinators</a></em> package:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Model&#39;</span> (<span class="ot">p ::</span> <span class="dt">Maybe</span> <span class="dt">Type</span>) (<span class="ot">s ::</span> <span class="dt">Maybe</span> <span class="dt">Type</span>) (<span class="ot">a ::</span> <span class="dt">Type</span>) (<span class="ot">b ::</span> <span class="dt">Type</span>) <span class="ot">=</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>       <span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Option</span> (<span class="dt">BVar</span> z) p</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Option</span> (<span class="dt">BVar</span> z) s</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> (<span class="dt">BVar</span> z b, <span class="dt">Option</span> (<span class="dt">BVar</span> z) s)</span></code></pre></div>
<p><code>Option f a</code> contains a value if <code>a</code> is <code>'Just</code>, and does not if <code>a</code> is <code>'Nothing</code>. More precisely, if <code>a</code> is <code>'Just b</code>, it will contain an <code>f b</code>. So if <code>p</code> is <code>'Just p'</code>, an <code>Option (BVar z) p</code> will contain a <code>BVar z p'</code>.</p>
<p>We can then re-define our previous types:</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Model</span>  p   <span class="ot">=</span> <span class="dt">Model&#39;</span> (<span class="dt">&#39;Just</span> p) <span class="dt">&#39;Nothing</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">ModelS</span> p s <span class="ot">=</span> <span class="dt">Model&#39;</span> (<span class="dt">&#39;Just</span> p) (<span class="dt">&#39;Just</span> s)</span></code></pre></div>
<p>And now that we have unified everything under the same type, we can write <code>mapS</code> that takes both stateful and non-stateful models, merge <code>(&lt;~)</code>, <code>(&lt;*~*)</code> and <code>(&lt;*~)</code>, etc., thanks to the power of dependent types.</p>
<p>As an added benefit, we also can unify parameterless functions too, which are often useful for composition:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Func</span> a b <span class="ot">=</span> <span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span> <span class="ot">=&gt;</span> <span class="dt">BVar</span> z a <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- or</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Func</span>     <span class="ot">=</span> <span class="dt">Model&#39;</span> <span class="dt">&#39;Nothing</span> <span class="dt">&#39;Nothing</span></span></code></pre></div>
<p>and we can use this with our unified <code>(&lt;~)</code> etc. to implement functions like <code>mapS</code> for free.</p>
<p>Note that dependent types and DataKind shenanigans aren’t necessary for any of this to work — it just has the possibility to make things even more seamless and unified.</p>
<h2 id="a-practical-framework">A Practical Framework</h2>
<p>At the end of it all, I really think that we don’t ever “need” a “neural network library” or a “neural network framework”. I don’t want to be hemmed into a specific opaque interface with a compositional API that requires me to learn new rules of composition or application or clunky object methods.</p>
<p>To be able to utilize this all today, you really only need a few things.</p>
<ul>
<li><p>A handful of small primitive models expressed as normal functions (like <code>linReg</code>, <code>fullyConnected</code>, <code>convolution</code>, <code>lstm</code> etc.)</p>
<p>The number of small primitives might be surprisingly small, given the combinators that we are able to write. However, basic fundamental primitives are important to be able to jump in and write any model you might need.</p></li>
<li><p>Some useful higher-order functions acting as utility combinators to common patterns of function composition, like <code>map</code>, <code>&lt;~</code>, etc.</p>
<p>These are never <em>required</em> — just convenient, since the functional API is already fully featured as it is. They are all defined “within the language”, in that you can always just implement them using normal function application and definition.</p>
<p>Having these handy will make certain workflows simpler, and also help to de-duplicate common patterns that come up often.</p>
<p>With these, models that seem seemingly very different can be defined in terms of simple combinator applications of other models, and that simple base models can be used to derive other models in surprising ways (like how a feed-forward layer can be turned into a recurrent layer or an autoregressive model)</p></li>
<li><p>A handy collection of (differentiable) <em>loss functions</em>; in this post, we only used squared error, but in other situations there might be other useful ones like cross-entropy. Just having common loss functions (and combinators to manipulate loss functions) at hand is useful for quick prototyping.</p>
<p>Loss functions can be combined with regularizing terms from parameters, if the regularization functions themselves are differentiable.</p></li>
<li><p>A handy collection of <em>optimizers</em>, allowing you to take a loss function, a set of samples, and a model, and return the optimal parameters using performant optimizers.</p>
<p>In this post we only used stochastic gradient descent, but other great optimizers out there are also worth having available, like momentum, adam, adagrad, etc.</p>
<p>These optimizers should be easily usable with different data streams for observations.</p></li>
</ul>
<p>That’s really it, I feel! Just the models <em>as functions</em>, the combinators, and methods to evaluate and train those functions. No “objects” defining layers as data (they’re not data, they’re functions!); just the full freedom of expressing a model as any old function you want.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<h2 id="a-path-forward">A Path Forward</h2>
<p>Thank you for making it to the end! I hope at this point you have been able to gain some appreciation for differential programming in a purely functional style, and see the sort of doors that this opens.</p>
<p>To tie it all together, I want to restate that a lot of things have to come together to make this all practical and useful. And, without any one of these, the whole thing would become clumsy.</p>
<ol type="1">
<li><p><strong>Functional Programming</strong>. Higher-order functions and combinators that take functions and return functions. Again, this allows us to draw from mathematical models directly, but also gives us full control over how we reshape, redefine, manipulate our models.</p>
<p>We aren’t forced to adhere to a limited API provided for our models; it all is just normal function application and higher-order functions — something that functional programming is very, very good at dealing with. In addition, writing our models as “just functions” means we can re-use functional programming staples like <code>foldl</code> (left folds) and <code>mapAccumL</code>.</p>
<p>Combinators are powerful — we saw how many models were just “combinator-applied” versions of simpler models.</p>
<p>Functional programming also forces us to consider state <em>explicitly</em>, instead of being an implicit part of the runtime. This makes combinators like <code>zeroState</code>, <code>unroll</code>, <code>recurrently</code>, and <code>lagged</code> possible. Because state is not a magic part of the system, it is something that we can <em>explicitly talk about</em> and <em>transform</em>, just as a first-class thing.</p></li>
<li><p><strong>Differentiable Programming</strong>. This should go without saying that nothing here would work without our functions all being differentiable. This is what allows us to train our models using gradient descent.</p>
<p>Again, I really don’t know if this is best when supported at the language/compiler level or at the library level. For this exploration, it is done at the library level, and I really don’t think it’s too bad!</p>
<p>In any case, I want to emphasize again that functional programming is a natural fit for differentiable programming, and the combination of them together is what makes this approach very powerful.</p></li>
<li><p><strong>Purely functional programming</strong> is, again, what lets us draw the correspondence between mathematical models and the models we describe here. And, as seen in the last part, this constraint forces us to consider alternatives to implicit state, which ends up yielding very fruitful results.</p>
<p>In impure languages, this is something that we have to always explicitly state as a property of our models. Purity is a <em>benefit</em>, especially when reasoning with stateful models. Tying the state of our models with the implicit state functionality of a programming language’s runtime system? Definitely a recipe for confusion and disaster.</p></li>
<li><p><strong>Strong expressive static type system</strong> with type inference makes this all possible to work with at the practical level.</p>
<p>I couldn’t imagine doing any of this without the help of a compiler that keeps track of your types for you. Most of our combinators manipulate state types of functions, many of them manipulate parameter types, and almost all of them manipulate input and output types. Having a compiler that keeps track of this for you and lets you ask questions about them is essential. The compiler also <em>helps you write your code</em> — if you leave a “typed hole” in your code, the compiler will tell you all of the combinators or values available that can fit inside that hole, and it usually is exactly the one you need.</p>
<p>And if you can state your desired model in terms of its types, sometimes the combinator applications and functions write themselves. They all act together as edges of puzzle pieces; and best of all, the compiler can tell you exactly what pieces you have available fit with what you have, automatically. Additionally, the process of thinking of types (within the language) can guide you in <em>writing</em> new combinators.</p>
<p>This method requires some complex types when you write non-trivial models; type inference frees you from the burden of keeping track of your parameter and state type, and has the compiler handle the work and the memory for you. And, at the end, when you have your finished model, your compiler will verify things like providing the right parameter to the right model, generating the correct parameter shape, etc.</p></li>
</ol>
<h3 id="comparisons">Comparisons</h3>
<p>Almost all current neural network and deep learning frameworks implement the full features that are described here. <em>tensorflow</em> and related libraries all provide a wrapper around essentially pure graph API. You can get started with all of this right away in python with tools like <a href="https://github.com/HIPS/autograd">autograd</a>.</p>
<p>What I’m really talking about isn’t specifically about Haskell or <em>backprop</em>; it’s more of a <em>functional approach</em> to these sorts of models. Currently right now, imperative API’s dominate the field. Sometimes when talking to friends, they can’t imagine how a functional or pure API would make sense.</p>
<p>The point of this series is to show that a functional and pure API with static types isn’t just possible, it’s immensely beneficial:</p>
<ul>
<li><p>There is no need for an imperative API, even as a wrapper. Even imperative API’s require an explicit assumption or promise of purity, anyway, that cannot be enforced — so what’s the point?</p></li>
<li><p><em>Layers as objects</em> (or as data) is not necessary. <em>Layers as functions</em> is the more faithful and extensible way. Almost all frameworks (like <em><a href="https://www.tensorflow.org/">tensorflow</a></em>, <em><a href="http://caffe.berkeleyvision.org/">caffe</a></em>, <em><a href="http://hackage.haskell.org/package/grenade-0.1.0">grenade</a></em>) fall into the this <a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.gc2fcdcce7_216_264">layer-as-data</a> mentality.</p>
<p>For example, what if we wanted to turn a model <code>a -&gt; b</code> (predicting b’s from a’s) into a model <code>[a] -&gt; [b]</code> (predicting the contents of a list of b’s from the contents of a list of a’s)?</p>
<p>In libraries like <em>tensorflow</em> and <em>caffe</em> and <em>grenade</em>, you might have to:</p>
<ol type="1">
<li>Create a new data structure</li>
<li>Use the API of the layer data structure to implement a bunch of methods for your data structure</li>
<li>Write a “forward” mode</li>
<li>Write a “backwards” mode</li>
<li>Define initializers for your data structure</li>
<li>Write trainers/nudgers for your data structure</li>
</ol>
<p>But in this system where layers are functions, this is just:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="ot">overList ::</span> <span class="dt">Model</span> p a b <span class="ot">-&gt;</span> <span class="dt">Model</span> p [a] [b]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>overList f p <span class="ot">=</span> <span class="fu">fmap</span> (f p)</span></code></pre></div>
<p>There is some minor boilerplate to make the types line up, but that’s essentially what it is. No special data structure, no abstract API to work with…just normal functions.</p></li>
<li><p>A functional and statically typed interface helps you, as a developer, <em>explore options</em> in ways that an imperative or untyped approach cannot. Removing the barrier between the math and the code helps with your thinking. It also guides how you look at combinators and creating models from others. Functional approaches also mean you have to think of no implicit state interactions behind the hood.</p></li>
</ul>
<p>In short, other similar frameworks might have some mix of of differentiable and “functional” programming, and some even with purity by contract. But it is specifically the combination of <em>all</em> of these (with static types) adds a lot of value in how you create and use and discover models.</p>
<p>One thing I excluded from discussion here is performance. Performance is going to be up to the system you use for differentiable programming, and so is not something I can meaningfully talk about. My posts here are simply about interface, and how they can help shape your thought when designing your own models.</p>
<h3 id="signing-off">Signing off</h3>
<p>In the end, this is all something that I’m still actively exploring. In a year now, my opinions might be very different. However, I’ve reached a point where I truly believe the future of differentiable programming and deep learning is functional, pure, and typed. For me, however, functional, pure, and typed differentiable programming is <em>my present</em>. Its contributions to my understanding of models and building new models is something that I take advantage of every day in my own modeling and research. I hope it can be helpful to you, as well!</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>This is the basis behind my work-in-progress <a href="https://github.com/mstksg/opto">opto</a> and <a href="https://github.com/mstksg/backprop-learn">backprop-learn</a> libraries.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div><footer><hr><div class="copy-content"><p>Hi, thanks for reading! You can reach me via email at <a href="mailto:justin@jle.im" class="email">justin@jle.im</a>, or at twitter at <a href="https://twitter.com/mstk">@mstk</a>! This post and all others are published under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/">CC-BY-NC-ND 3.0</a> license. Corrections and edits via pull request are welcome and encouraged at <a href="https://github.com/mstksg/inCode">the source repository</a>.</p>
<p>If you feel inclined, or this post was particularly helpful for you, why not consider <a href="https://www.patreon.com/justinle/overview">supporting me on Patreon</a>, or a <a href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">BTC donation</a>? :)</p></div><div class="clear"></div><ul class="entry-series"><li><div>This entry is a part of a series called <b>&quot;Functional Models&quot;</b>.  Find the rest of the entries in this series at its <a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series" title="+Functional Models"> series history</a>.</div></li></ul><ul class="tag-list"><li><a href="https://blog.jle.im/entries/tagged/backprop.html" class="tag-a-tag">#backprop</a></li><li><a href="https://blog.jle.im/entries/tagged/deep-learning.html" class="tag-a-tag">#deep learning</a></li><li><a href="https://blog.jle.im/entries/tagged/differentiable-programming.html" class="tag-a-tag">#differentiable programming</a></li><li><a href="https://blog.jle.im/entries/tagged/machine-learning.html" class="tag-a-tag">#machine learning</a></li><li><a href="https://blog.jle.im/entries/tagged/modeling.html" class="tag-a-tag">#modeling</a></li><li><a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li><li><a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series">+Functional Models</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="https://www.reddit.com/submit" onclick="window.location = &#39;https://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="https://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="prev-entry-link">&larr; <a href="https://blog.jle.im/entry/purely-functional-typed-models-2.html">A Purely Functional Typed Approach to Trainable Models (Part 2)</a> (Previous)</li><li class="next-entry-link">(Next) <a href="https://blog.jle.im/entry/in-memory-of-ertugrul-soylemez.html">In Memory of Ertugrul Söylemez (1985 - 2018)</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blog.jle.im/entry/purely-functional-typed-models-3.html';
    this.page.identifier = 'functional-models-3';
};
(function() {
    var d = document, s = d.createElement('script');
    s.src = '//incode.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2020 Justin Le <span class="license-link">(<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" class="license">CC-BY-NC-ND 3.0</a>)</span></div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/mstksg">Github</a></li><li><a class="social-follow-twitch" title="Watch me on Twitch!" href="https://www.twitch.tv/justin_l">Twitch</a></li><li><a class="social-follow-patreon" title="Support me on Patreon!" href="https://www.patreon.com/justinle/overview">Patreon</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/lejustin">LinkedIn</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>