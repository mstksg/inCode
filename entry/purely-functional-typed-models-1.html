<!DOCTYPE HTML>
<html><head><title>A Purely Functional Typed Approach to Trainable Models (Part 1) · in Code</title><meta name="description" content="Weblog of Justin Le, covering various adventures in programming and explorations in the worlds of computation physics, and knowledge.
"><meta http-equiv="Content-Type" content="text/html;charset=utf-8"><meta name="viewport" content="width=device-width,initial-scale=1.0"><meta name="flattr:id" content="3p9jqr"><meta property="og:site_name" content="in Code"><meta property="og:description" content="With the release of backprop, I’ve been exploring the space of parameterized models of all sorts, from linear and logistic regression and other statistical models to artificial neural networks, feed-forward and recurrent (stateful). I wanted to see to what extent we can really apply automatic differentiation and iterative gradient decent-based training to all of these different models. Basically, I wanted to see how far we can take differentiable programming (a la Yann LeCun) as a paradigm for writing trainable models. Building on other writers, I’m starting to see a picture unifying all of these models, painted in the language of purely typed functional programming. I’m already applying these to models I’m using in real life and in my research, and I thought I’d take some time to put my thoughts to writing in case anyone else finds these illuminating or useful. As a big picture, I really believe that a purely functional typed approach to differentiable programming is the way to move forward in the future for models like artificial neural networks. In this light, the drawbacks of object-oriented and imperative approaches becomes very apparent. I’m not the first person to attempt to build a conceptual framework for these types of models in a purely functional typed sense – Christopher Olah’s famous post wrote a great piece in 2015 that this post heavily builds off of, and is definitely worth a read! We’ll be taking some of his ideas and seeing how they work in real code! This will be a three-part series, and the intended audience is people who have a passing familiarity with statistical modeling or machine learning/deep learning. The code in these posts is written in Haskell, using the backprop and hmatrix (with hmatrix-backprop) libraries, but the main themes and messages won’t be about haskell, but rather about differentiable programming in a purely functional typed setting in general. This isn’t a Haskell post as much as it is an exploration, using Haskell syntax/libraries to implement the points. The backprop library is roughly equivalent to autograd in python, so all of the ideas apply there as well. The source code for the written code in this module is available on github, if you want to follow along!"><meta property="og:type" content="article"><meta property="og:title" content="A Purely Functional Typed Approach to Trainable Models (Part 1)"><meta property="og:image" content="https://blog.jle.im/img/site_logo.jpg"><meta property="og:locale" content="en_US"><meta property="og:url" content="https://blog.jle.im/entry/purely-functional-typed-models-1.html"><meta name="twitter:card" content="summary"><meta name="twitter:creator:id" content="mstk"><link rel="author" href="https://plus.google.com/107705320197444500140"><link rel="alternate" type="application/rss+xml" title="in Code (RSS Feed)" href="http://feeds.feedburner.com/incodeblog"><link rel="canonical" href="https://blog.jle.im/entry/purely-functional-typed-models-1.html"><link href="https://blog.jle.im/favicon.ico" rel="shortcut icon"><link href="https://blog.jle.im/css/toast.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/font.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/main.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/page/entry.css" rel="stylesheet" type="text/css"><link href="https://blog.jle.im/css/pygments.css" rel="stylesheet" type="text/css"><script type="text/javascript">var page_data = {};
var disqus_shortname='incode';
</script><script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-443711-8', 'jle.im');
ga('send', 'pageview');
</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script><script type="text/javascript" src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script><script type="text/javascript" src="//s7.addthis.com/js/300/addthis_widget.js#pubid=ra-5234d67a6b68dcd4"></script><script type="text/javascript" src="https://blog.jle.im/js/page/entry_toc.js"></script><script type="text/javascript" src="https://blog.jle.im/js/disqus_count.js"></script><script type="text/javascript" src="https://blog.jle.im/js/social.js"></script><script type="text/javascript" src="https://blog.jle.im/js/jquery/jquery.toc.js"></script><script type="text/javascript" src="https://blog.jle.im/purescript/entry.js"></script></head><body><div id="fb-root"><script>(function(d, s, id) {
 var js, fjs = d.getElementsByTagName(s)[0];
 if (d.getElementById(id)) return;
 js = d.createElement(s); js.id = id;
 js.src = "//connect.facebook.net/en_US/all.js#xfbml=1&appId=641852699171929";
 fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script></div><div id="header-container"><div id="navbar-container" class="tile"><nav id="navbar-content"><div class="nav-info"><h1 class="site-title"><a href="https://blog.jle.im/" class="nav-title">in Code</a></h1><span class="nav-author">Justin Le</span></div><ul class="nav-links"><li><a href="https://blog.jle.im/">home</a></li><li><a href="https://blog.jle.im/entries.html">archives</a></li><li><a href="https://cv.jle.im">cv</a></li><div class="clear"></div></ul></nav></div><div id="header-content"></div></div><div id="body-container" class="container"><div id="main-container" class="grid"><div class="entry-section unit span-grid" role="main"><article class="tile article"><header><h1 id="title">A Purely Functional Typed Approach to Trainable Models (Part 1)</h1><p class="entry-info">by <a class="author" href="https://blog.jle.im/">Justin Le</a><span class="info-separator"> &diams; </span><time datetime="2018-05-14T12:16:08Z" pubdate="" class="pubdate">Monday May 14, 2018</time></p><p><span class="source-info"><a class="source-link" href="https://github.com/mstksg/inCode/tree/master/copy/entries/functional-models-1.md">Source</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://github.com/mstksg/inCode/tree/gh-pages/entry/purely-functional-typed-models-1.md">Markdown</a><span class="info-separator"> &diams; </span><a class="source-link" href="https://blog.jle.im/entry/purely-functional-typed-models-1.tex">LaTeX</a><span class="info-separator"> &diams; </span></span>Posted in <a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category" title="Functional, pure, non-strict, statically and strongly typed, natively
compiled...really just the king of great languages.
">Haskell</a><span class="info-separator"> &diams; </span><a class="comment-link" href="#disqus_thread">Comments</a></p></header><hr><aside class="contents-container"><h5 id="contents-header">Contents</h5><div id="toc"></div></aside><div class="main-content copy-content"><p>With the release of <a href="http://hackage.haskell.org/package/backprop">backprop</a>, I’ve been exploring the space of parameterized models of all sorts, from linear and logistic regression and other statistical models to artificial neural networks, feed-forward and recurrent (stateful). I wanted to see to what extent we can really apply automatic differentiation and iterative gradient decent-based training to all of these different models. Basically, I wanted to see how far we can take <em>differentiable programming</em> (a la <a href="https://www.facebook.com/yann.lecun/posts/10155003011462143">Yann LeCun</a>) as a paradigm for writing trainable models.</p>
<p>Building on other writers, I’m starting to see a picture unifying all of these models, painted in the language of purely typed functional programming. I’m already applying these to models I’m using in real life and in my research, and I thought I’d take some time to put my thoughts to writing in case anyone else finds these illuminating or useful.</p>
<p>As a big picture, I really believe that a purely functional typed approach to differentiable programming is <em>the</em> way to move forward in the future for models like artificial neural networks. In this light, the drawbacks of object-oriented and imperative approaches becomes very apparent.</p>
<p>I’m not the first person to attempt to build a conceptual framework for these types of models in a purely functional typed sense – <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">Christopher Olah’s famous post</a> wrote a great piece in 2015 that this post heavily builds off of, and is definitely worth a read! We’ll be taking some of his ideas and seeing how they work in real code!</p>
<p>This will be a three-part series, and the intended audience is people who have a passing familiarity with statistical modeling or machine learning/deep learning. The code in these posts is written in Haskell, using the <a href="http://hackage.haskell.org/package/backprop">backprop</a> and <a href="http://hackage.haskell.org/package/hmatrix">hmatrix</a> (with <a href="http://hackage.haskell.org/package/hmatrix-backprop">hmatrix-backprop</a>) libraries, but the main themes and messages won’t be <em>about</em> haskell, but rather about differentiable programming in a purely functional typed setting in general. This isn’t a Haskell post as much as it is an exploration, using Haskell syntax/libraries to implement the points. The <em>backprop</em> library is roughly equivalent to <a href="https://github.com/HIPS/autograd">autograd</a> in python, so all of the ideas apply there as well.</p>
<p>The source code for the written code in this module is available <a href="https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs">on github</a>, if you want to follow along!</p>
<h2 id="essence-of-a-model">Essence of a Model</h2>
<p>For the purpose of this post, a <em>parameterized model</em> is a function from some input “question” (predictor, independent variable) to some output “answer” (predictand, dependent variable)</p>
<p>Notationally, we might write it as a function:</p>
<p><span class="math display">\[
f_p(x) = y
\]</span></p>
<p>The important thing is that, for every choice of <em>parameterization</em> <span class="math inline">\(p\)</span>, we get a <em>different function</em> <span class="math inline">\(f_p(x)\)</span>.</p>
<p>For example, you might want to write a model that, when given an email, outputs whether or not that email is spam.</p>
<p>The parameterization <em>p</em> is some piece of data that we tweak to produce a different <span class="math inline">\(f_p(x)\)</span>. So, “training” (or “learning”, or “estimating”) a model is a process of picking the <span class="math inline">\(p\)</span> that gives the “correct” function <span class="math inline">\(f_p(x)\)</span> — that is, the function that accurately predicts spam or whatever thing you are trying to predict.</p>
<p>For example, for <a href="https://en.wikipedia.org/wiki/Linear_regression">linear regression</a>, you are trying to “fit” your <span class="math inline">\((x, y)\)</span> data points to some function <span class="math inline">\(f(x) = \beta + \alpha x\)</span>. The <em>parameters</em> are <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, the <em>input</em> is <span class="math inline">\(x\)</span>, and the <em>output</em> is <span class="math inline">\(\beta + \alpha
x\)</span>.</p>
<p>As it so happens, a <span class="math inline">\(f_p(x)\)</span> is really just a “partially applied” <span class="math inline">\(f(p,x)\)</span>. Imagining that function, it has type:<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p><span class="math display">\[
f : (P \times A) \rightarrow B
\]</span></p>
<p>If we <a href="https://en.wikipedia.org/wiki/Currying">curry</a> this, we get the original model representation we talked about:</p>
<p><span class="math display">\[
f : P \rightarrow (A \rightarrow B)
\]</span></p>
<h3 id="optimizing-models-with-observations">Optimizing Models with Observations</h3>
<p>Something interesting happens if we flip the script. What if, instead of <span class="math inline">\(f_p(x)\)</span>, we talked about <span class="math inline">\(f_x(p)\)</span>? That is, we fix the input and vary the parameter, and see what type of outputs we get for the same output while we vary the parameter?</p>
<p>If we have an “expected output” for our input, then one thing we can do is look at <span class="math inline">\(f_x(p)\)</span> and see when the result is close to <span class="math inline">\(y_x\)</span> (the expected output of our model when given <span class="math inline">\(x\)</span>).</p>
<p>In fact, we can turn this into an optimization problem by trying to pick <span class="math inline">\(p\)</span> that minimizes the difference between <span class="math inline">\(f_x(p)\)</span> and <span class="math inline">\(y_x\)</span>. We can say that our model with parameter <span class="math inline">\(p\)</span> predicts <span class="math inline">\(y_x\)</span> the best when we minimize:</p>
<p><span class="math display">\[
(f_x(p) - y_x)^2
\]</span></p>
<p>If we minimize the squared error between the result of picking the parameter and the expected result, we find the best parameters for that given input!</p>
<p>In general, picking the best parameter for the model involves picking the <span class="math inline">\(p\)</span> that minimizes the relationship</p>
<p><span class="math display">\[
\text{loss}(y_x, f_x(p))
\]</span></p>
<p>Where <span class="math inline">\(\text{loss} : B \times B \rightarrow \mathbb{R}\)</span> gives a measure of “how badly” the model result differs from the expected target. Common loss functions include squared error, cross-entropy, etc.</p>
<p>This gives us a supervised way to train any model: if we have enough observations (<span class="math inline">\((x, y_x)\)</span> pairs) we can just pick a <span class="math inline">\(p\)</span> that does its best to make the loss between all observations as small as possible.</p>
<h3 id="stochastic-gradient-descent">Stochastic Gradient Descent</h3>
<p>If our model is a <em>differentiable function</em>, then we have a nice tool we can use: <em>stochastic gradient descent</em> (SGD).</p>
<p>That is, we can always calculate the <em>gradient</em> of the loss function with respect to our parameters. This gives us the direction we can “nudge” our parameters to make the loss bigger or smaller.</p>
<p>That is, if we get the <em>gradient</em> of the loss with respect to <span class="math inline">\(p\)</span> (<span class="math inline">\(\nabla_p
\text{loss}(f_x(p), y_x)\)</span>), we now have a nice iterative way to “train” our model:</p>
<ol type="1">
<li>Start with an initial guess at the parameter</li>
<li>Look at a random <span class="math inline">\((x, y_x)\)</span> observation pair.</li>
<li>Compute the gradient <span class="math inline">\(\nabla_p \text{loss}(f_x(p), y_x)\)</span> of our current <span class="math inline">\(p\)</span>, which tells us a direction we can “nudge” <span class="math inline">\(p\)</span> in to make the loss smaller.</li>
<li>Nudge <span class="math inline">\(p\)</span> in that direction</li>
<li>Repeat from #2 until satisfied</li>
</ol>
<p>With every new observation, we see how we can nudge the parameter to make the model more accurate, and then we perform that nudge. At the end of it all, we wind up just the right <code>p</code> to model the relationship between our observation pairs.</p>
<h2 id="functional-implementation">Functional Implementation</h2>
<p>What I described naturally lends to a functional implementation. That’s because, in this light, a model is nothing more than a curried function (a function returning a function). A model that is trainable using SGD is simply a differentiable function.</p>
<p>Using the <em><a href="http://hackage.haskell.org/package/backprop">backprop</a></em> library, we can write these differentiable functions as normal functions.</p>
<p>Let’s pick a type for our models. A model from type <code>a</code> to type <code>b</code> with parameter <code>p</code> can be written as the type synonym</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Model</span> p a b <span class="ot">=</span> p <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> b</span></code></pre></div>
<p>Not normally differentiable, but we can make it a differentiable function by having it work with <code>BVar z p</code> and <code>BVar z a</code> (<code>BVar</code>s containing those values) instead:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L54-L57</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">type</span> <span class="dt">Model</span> p a b <span class="ot">=</span> <span class="kw">forall</span> z<span class="op">.</span> <span class="dt">Reifies</span> z <span class="dt">W</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                <span class="ot">=&gt;</span> <span class="dt">BVar</span> z p</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                <span class="ot">-&gt;</span> <span class="dt">BVar</span> z a</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>                <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b</span></code></pre></div>
<p>This is a RankN <em>type synonym</em>, which is saying that a <code>Model p a b</code> is just a type synonym for a differentiable <code>BVar z p -&gt; BVar z a -&gt; BVar z b</code>. The <code>Reifies z W</code> is just a constraint that allows for backpropagation of <code>BVar</code>s.</p>
<p>We can write a simple linear regression model:</p>
<p><span class="math display">\[
f_{\alpha, \beta}(x) = \beta x + \alpha
\]</span></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L50-L371</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> a <span class="op">:&amp;</span> b <span class="ot">=</span> <span class="op">!</span>a <span class="op">:&amp;</span> <span class="op">!</span>b</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="kw">pattern</span><span class="ot"> (:&amp;&amp;) ::</span> (<span class="dt">Backprop</span> a, <span class="dt">Backprop</span> b, <span class="dt">Reifies</span> z <span class="dt">W</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>              <span class="ot">=&gt;</span> <span class="dt">BVar</span> z a <span class="ot">-&gt;</span> <span class="dt">BVar</span> z b <span class="ot">-&gt;</span> <span class="dt">BVar</span> z (a <span class="op">:&amp;</span> b)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="ot">linReg ::</span> <span class="dt">Model</span> (<span class="dt">Double</span> <span class="op">:&amp;</span> <span class="dt">Double</span>) <span class="dt">Double</span> <span class="dt">Double</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>linReg (a <span class="op">:&amp;&amp;</span> b) x <span class="ot">=</span> b <span class="op">*</span> x <span class="op">+</span> a</span></code></pre></div>
<p>A couple things going on here to help us do things smoothly:</p>
<ul>
<li><p>We define a custom tuple data type <code>:&amp;</code>; backprop works with normal tuples, but using a custom tuple with a <code>Num</code> instance will come in handy later for training models.</p></li>
<li><p>We define a pattern synonym <code>:&amp;&amp;</code> that lets us “pattern match out” <code>BVar</code>s of that tuple type. So if we have a <code>BVar z (a :&amp; b)</code> (a <code>BVar</code> containing a tuple), then matching on <code>(x :&amp;&amp; y)</code> will give us <code>x :: BVar z     a</code> and <code>y :: BVar z b</code>.</p></li>
<li><p>With that, we define <code>linReg</code>, whose parameters are a <code>Double :&amp; Double</code>, a tuple the two parameters <code>a</code> and <code>b</code>. After pattern matching out the contents, we just write the linear regression formula — <code>b * x + a</code>. We can use normal numeric operations like <code>*</code> and <code>+</code> because <code>BVar</code>s have a <code>Num</code> instance.</p></li>
</ul>
<p>We can <em>run</em> <code>linReg</code> using <code>evalBP2</code>:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 linReg (<span class="fl">0.3</span> <span class="op">:&amp;</span> (<span class="op">-</span><span class="fl">0.1</span>)) <span class="dv">5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="op">-</span><span class="fl">0.2</span>        <span class="co">-- (-0.1) * 5 + 0.3</span></span></code></pre></div>
<p>But the neat thing is that we can also get the gradient of the parameters, too, if we identify a loss function:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p><span class="math display">\[
\nabla_p (f(p, x) - y_x)^2
\]</span></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L62-L70</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>squaredErrorGrad</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Backprop</span> p, <span class="dt">Backprop</span> b, <span class="dt">Num</span> b)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> p a b      <span class="co">-- ^ Model</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> a                <span class="co">-- ^ Observed input</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> b                <span class="co">-- ^ Observed output</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> p                <span class="co">-- ^ Parameter guess</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> p                <span class="co">-- ^ Gradient</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>squaredErrorGrad f x targ <span class="ot">=</span> gradBP <span class="op">$</span> \p <span class="ot">-&gt;</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    (f p (auto x) <span class="op">-</span> auto targ) <span class="op">^</span> <span class="dv">2</span></span></code></pre></div>
<p>We use <code>auto :: a -&gt; BVar z a</code>, to lift a normal value to a <code>BVar</code> holding that value, since our model <code>f</code> takes <code>BVar</code>s.</p>
<p>And finally, we can train it using stochastic gradient descent, with just a simple fold over all observations:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L72-L78</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>trainModel</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Fractional</span> p, <span class="dt">Backprop</span> p, <span class="dt">Num</span> b, <span class="dt">Backprop</span> b)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> p a b      <span class="co">-- ^ model to train</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> p                <span class="co">-- ^ initial parameter guess</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> [(a,b)]          <span class="co">-- ^ list of observations</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> p                <span class="co">-- ^ updated parameter guess</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>trainModel f <span class="ot">=</span> foldl&#39; <span class="op">$</span> \p (x,y) <span class="ot">-&gt;</span> p <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> squaredErrorGrad f x y p</span></code></pre></div>
<p>For convenience, we can define a <code>Random</code> instance for our tuple type using the <em><a href="http://hackage.haskell.org/package/random">random</a></em> library and make a wrapper that uses <code>IO</code> to generate a random initial parameter:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L80-L87</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>trainModelIO</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Fractional</span> p, <span class="dt">Backprop</span> p, <span class="dt">Num</span> b, <span class="dt">Backprop</span> b, <span class="dt">Random</span> p)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> p a b      <span class="co">-- ^ model to train</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> [(a,b)]          <span class="co">-- ^ list of observations</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">IO</span> p             <span class="co">-- ^ parameter guess</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>trainModelIO m xs <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    p0 <span class="ot">&lt;-</span> (<span class="op">/</span> <span class="dv">10</span>) <span class="op">.</span> <span class="fu">subtract</span> <span class="fl">0.5</span> <span class="op">&lt;$&gt;</span> randomIO</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    <span class="fu">return</span> <span class="op">$</span> trainModel m p0 xs</span></code></pre></div>
<p>Let’s train our linear regression model to fit the points <code>(1,1)</code>, <code>(2,3)</code>, <code>(3,5)</code>, <code>(4,7)</code>, and <code>(5,9)</code>! This should follow <span class="math inline">\(f(x) = 2 x - 1\)</span>, or <span class="math inline">\(\alpha = -1,\, \beta = 2\)</span>:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> samps <span class="ot">=</span> [(<span class="dv">1</span>,<span class="dv">1</span>),(<span class="dv">2</span>,<span class="dv">3</span>),(<span class="dv">3</span>,<span class="dv">5</span>),(<span class="dv">4</span>,<span class="dv">7</span>),(<span class="dv">5</span>,<span class="dv">9</span>)]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> trainModelIO linReg <span class="op">$</span> <span class="fu">take</span> <span class="dv">5000</span> (<span class="fu">cycle</span> samps)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>(<span class="op">-</span><span class="fl">1.0000000000000024</span>) <span class="op">:&amp;</span> <span class="fl">2.0000000000000036</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co">-- roughly:</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>(<span class="op">-</span><span class="fl">1.0</span>) <span class="op">:&amp;</span> <span class="fl">2.0</span></span></code></pre></div>
<p>Neat — after going through all of those observations a thousand times, the model nudges itself all the way to the right parameters to fit our model!</p>
<p>The important takeaway is that all we specified was the <em>function</em> of the model itself. The training part all follows automatically.</p>
<h3 id="feed-forward-neural-network">Feed-forward Neural Network</h3>
<p>Here’s another example: a fully-connected feed-forward neural network layer.</p>
<p>We can start with a single layer. The model here will also take two parameters (a weight matrix and a bias vector), take in a vector, and output a vector.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">import</span> <span class="dt">Numeric.LinearAlgebra.Static.Backprop</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L94-L105</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="ot">logistic ::</span> <span class="dt">Floating</span> a <span class="ot">=&gt;</span> a <span class="ot">-&gt;</span> a</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>logistic x <span class="ot">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> <span class="fu">exp</span> (<span class="op">-</span>x))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>feedForwardLog</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> (<span class="dt">L</span> o i <span class="op">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>feedForwardLog (w <span class="op">:&amp;&amp;</span> b) x <span class="ot">=</span> logistic (w <span class="op">#&gt;</span> x <span class="op">+</span> b)</span></code></pre></div>
<p>Here we use the <code>L n m</code> (an n-by-m matrix) and <code>R n</code> (an n-vector) types from the <em>hmatrix</em> library, and <code>#&gt;</code> for backprop-aware matrix-vector multiplication.</p>
<p>Let’s try training a model to learn the simple <a href="https://en.wikipedia.org/wiki/Logical_conjunction">logical “AND”</a>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> <span class="kw">import</span> <span class="kw">qualified</span> <span class="dt">Numeric.LinearAlgebra.Static</span> <span class="kw">as</span> <span class="dt">H</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> samps <span class="ot">=</span> [(H.vec2 <span class="dv">0</span> <span class="dv">0</span>, <span class="dv">0</span>), (H.vec2 <span class="dv">1</span> <span class="dv">0</span>, <span class="dv">0</span>), (H.vec2 <span class="dv">0</span> <span class="dv">1</span>, <span class="dv">0</span>), (H.vec2 <span class="dv">1</span> <span class="dv">1</span>, <span class="dv">1</span>)]</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> trained <span class="ot">&lt;-</span> trainModelIO feedForwardLog <span class="op">$</span> <span class="fu">take</span> <span class="dv">10000</span> (<span class="fu">cycle</span> samps)</span></code></pre></div>
<p>We have our trained parameters! Let’s see if they actually model “AND”?</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 feedForwardLog trained (H.vec2 <span class="dv">0</span> <span class="dv">0</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>(<span class="fl">7.468471910660985e-5</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)       <span class="co">-- 0.0</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 feedForwardLog trained (H.vec2 <span class="dv">1</span> <span class="dv">0</span>)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>(<span class="fl">3.816205998697482e-2</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)       <span class="co">-- 0.0</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 feedForwardLog trained (H.vec2 <span class="dv">0</span> <span class="dv">1</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>(<span class="fl">3.817490115313559e-2</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)       <span class="co">-- 0.0</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 feedForwardLog trained (H.vec2 <span class="dv">1</span> <span class="dv">1</span>)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>(<span class="fl">0.9547178031665701</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)         <span class="co">-- 1.0</span></span></code></pre></div>
<p>Close enough for me!</p>
<p>If we inspect the arrived-at parameters, we can peek into the neural network’s brain:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> trained</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>(matrix</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">4.652034474187562</span>, <span class="fl">4.65355702367007</span> ]<span class="ot"> ::</span> <span class="dt">L</span> <span class="dv">1</span> <span class="dv">2</span>) <span class="op">:&amp;</span> (<span class="op">-</span><span class="fl">7.073724083776028</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)</span></code></pre></div>
<p>It seems like there is a heavy negative bias, and that each of the inputs makes some contribution that is slightly more than half of the negative bias; the end goal is that one of the inputs alone makes no dent, but only if both inputs are “on”, the output can overcome the negative bias.</p>
<p>The network was able to arrive that this configuration just by exploring the gradient of our differentiable function!</p>
<h3 id="functional-composition">Functional composition</h3>
<p>Because our functions are simply just <em>normal functions</em>, we can create new, complex models from simpler ones using just functional composition.</p>
<p>For example, we can map the result of a model to create a new model. Here, we compose <code>linReg ab</code> (linear regression with parameter <code>ab</code>) with the logistic function to create a <em><a href="https://en.wikipedia.org/wiki/Logistic_regression">logistic regression</a></em> model.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L119-L120</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ot">logReg ::</span> <span class="dt">Model</span> (<span class="dt">Double</span> <span class="op">:&amp;</span> <span class="dt">Double</span>) <span class="dt">Double</span> <span class="dt">Double</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>logReg ab <span class="ot">=</span> logistic <span class="op">.</span> linReg ab</span></code></pre></div>
<p>Here, we use function composition <code>(.)</code>, one of the most common combinators in Haskell, saying that <code>(f . g) x = f (g x)</code>.</p>
<p>We could have even written our <code>feedForwardLog</code> without its activation function:</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L97-L100</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>feedForward</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> (<span class="dt">L</span> o i <span class="op">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>feedForward (w <span class="op">:&amp;&amp;</span> b) x <span class="ot">=</span> w <span class="op">#&gt;</span> x <span class="op">+</span> b</span></code></pre></div>
<p>And now we can swap out activation functions using simple function composition:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L122-L125</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>feedForwardLog&#39;</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> (<span class="dt">L</span> o i <span class="op">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>feedForwardLog&#39; wb <span class="ot">=</span> logistic <span class="op">.</span> feedForward wb</span></code></pre></div>
<p>Maybe even a <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> classifier!</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L127-L135</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="ot">softMax ::</span> (<span class="dt">Reifies</span> z <span class="dt">W</span>, <span class="dt">KnownNat</span> n) <span class="ot">=&gt;</span> <span class="dt">BVar</span> z (<span class="dt">R</span> n) <span class="ot">-&gt;</span> <span class="dt">BVar</span> z (<span class="dt">R</span> n)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>softMax x <span class="ot">=</span> konst (<span class="dv">1</span> <span class="op">/</span> sumElements expx) <span class="op">*</span> expx</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>  <span class="kw">where</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    expx <span class="ot">=</span> <span class="fu">exp</span> x</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>feedForwardSoftMax</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">KnownNat</span> i, <span class="dt">KnownNat</span> o)</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span> (<span class="dt">L</span> o i <span class="op">:&amp;</span> <span class="dt">R</span> o) (<span class="dt">R</span> i) (<span class="dt">R</span> o)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>feedForwardSoftMax wb <span class="ot">=</span> softMax <span class="op">.</span> feedForward wb</span></code></pre></div>
<p>We can even write a function to <em>compose</em> two models, keeping their two original parameters separate:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- source: https://github.com/mstksg/inCode/tree/master/code-samples/functional-models/model.hs#L137-L143</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>(<span class="op">&lt;~</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="ot">    ::</span> (<span class="dt">Backprop</span> p, <span class="dt">Backprop</span> q)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    <span class="ot">=&gt;</span> <span class="dt">Model</span>  p       b c</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Model</span>       q  a b</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="ot">-&gt;</span> <span class="dt">Model</span> (p <span class="op">:&amp;</span> q) a c</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>(f <span class="op">&lt;~</span> g) (p <span class="op">:&amp;&amp;</span> q) <span class="ot">=</span> f p <span class="op">.</span> g q</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="kw">infixr</span> <span class="dv">8</span> <span class="op">&lt;~</span></span></code></pre></div>
<p>And now we have a way to chain models! Maybe even make a multiple-layer neural network? Let’s see if we can get a two-layer model to learn <a href="https://en.wikipedia.org/wiki/Exclusive_or">XOR</a> …</p>
<p>Our model is two feed-forward layers with logistic activation functions, with 4 hidden layer units:</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> <span class="kw">let</span><span class="ot"> twoLayer ::</span> <span class="dt">Model</span> _ (<span class="dt">R</span> <span class="dv">2</span>) (<span class="dt">R</span> <span class="dv">1</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>          twoLayer <span class="ot">=</span> feedForwardLog&#39; <span class="op">@</span><span class="dv">4</span> <span class="op">&lt;~</span> feedForwardLog&#39;</span></code></pre></div>
<p>Note we use type application syntax (the <code>@</code>) to specify the input/output dimensions of <code>feedForwardLog'</code> to set our hidden layer size; when we write <code>feedForwardLog' @4</code>, it means to set the <code>i</code> type variable to <code>4</code>. We also use <code>_</code> type wildcard syntax because we want to just let the compiler infer the type of the model parameter for us instead of explicitly writing it out ourselves.</p>
<p>We can train it on sample points:</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> <span class="kw">let</span><span class="ot"> samps ::</span> [(<span class="dt">R</span> <span class="dv">2</span>, <span class="dt">R</span> <span class="dv">1</span>)]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>          samps <span class="ot">=</span> [(H.vec2 <span class="dv">0</span> <span class="dv">0</span>, <span class="dv">0</span>), (H.vec2 <span class="dv">1</span> <span class="dv">0</span>, <span class="dv">1</span>), (H.vec2 <span class="dv">0</span> <span class="dv">1</span>, <span class="dv">1</span>), (H.vec2 <span class="dv">1</span> <span class="dv">1</span>, <span class="dv">0</span>)]</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> trained <span class="ot">&lt;-</span> trainModelIO twoLayer <span class="op">$</span> <span class="fu">take</span> <span class="dv">50000</span> (<span class="fu">cycle</span> samps)</span></code></pre></div>
<p>Trained. Now, does it model “XOR”?</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 twoLayer trained (H.vec2 <span class="dv">0</span> <span class="dv">0</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>(<span class="fl">3.0812844350410647e-2</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)          <span class="co">-- 0.0</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 twoLayer trained (H.vec2 <span class="dv">1</span> <span class="dv">0</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>(<span class="fl">0.959153369985914</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)              <span class="co">-- 1.0</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 twoLayer trained (H.vec2 <span class="dv">0</span> <span class="dv">1</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>(<span class="fl">0.9834757090696419</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)             <span class="co">-- 1.0</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> evalBP2 twoLayer trained (H.vec2 <span class="dv">1</span> <span class="dv">1</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>(<span class="fl">3.6846467867668035e-2</span><span class="ot"> ::</span> <span class="dt">R</span> <span class="dv">1</span>)          <span class="co">-- 0.0</span></span></code></pre></div>
<p>Not bad!</p>
<h2 id="just-functions">Just Functions</h2>
<p>We just built a working neural network using normal function composition and simple combinators. No need for any objects or mutability or fancy explicit graphs. Just pure, typed functions! Why would you ever bring anything imperative into this?</p>
<p>You can build a lot with just these tools alone. By using primitive models and the various combinators, you can create autoencoders, nonlinear regressions, convolutional neural networks, multi-layered neural networks, generative adversarial networks…you can create complex “graphs” of networks that fork and re-combine with themselves.</p>
<p>The nice thing is that these are all just regular (Rank-2) functions, so…you have two models? Just compose their functions like normal functions!</p>
<p>It is tempting to look at something like</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>feedForwardLog <span class="op">@</span><span class="dv">4</span> <span class="op">&lt;~</span> feedForwardLog</span></code></pre></div>
<p>and think of it as some sort of abstract, opaque data type with magic inside. After all, “layers” are “data”, right? But, at the end of the day, it’s all just:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>\(p <span class="op">:&amp;&amp;</span> q) <span class="ot">-&gt;</span> feedForwardLog <span class="op">@</span><span class="dv">4</span> p <span class="op">.</span> feedForwardLog q</span></code></pre></div>
<p>Just normal function composition – we’re really just defining the <em>function</em> itself, and <em>backprop</em> turns that function into a trainable model.</p>
<p>In the past I’ve talked about <a href="https://blog.jle.im/entry/practical-dependent-types-in-haskell-1.html">layers as data</a>, and neural network libraries like <a href="http://hackage.haskell.org/package/grenade-0.1.0">grenade</a> let you manipulate neural network layers in a composable way. My previous attempts at neural networks like <a href="https://github.com/mstksg/tensor-ops">tensor-ops</a> also force a similar structure of composition of data types. Frameworks like <em><a href="https://www.tensorflow.org/">tensorflow</a></em> and <em><a href="http://caffe.berkeleyvision.org/">caffe</a></em> also treat <a href="https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.gc2fcdcce7_216_264">layer as data</a>. However, I feel this is a bit limiting.</p>
<p>You are forced to “compose” your layers in only the ways that the API of the data type gives you. You have to use the data type’s “function composition” functions, or its special “mapping” functions…and for weird things like forking compositions like <code>\x -&gt; f (g x) (h x)</code> you have to learn how the data type offers such an API.</p>
<p>However, such a crazy composition here is “trivial” – it’s all just normal functions, so you can just literally write out code like <code>\x -&gt; f (g x) (h x)</code> (or something very close). You don’t have to learn any rules of special “layer” data types. Layers aren’t matrices or “data” — they’re functions. Not just abstractly, but literally. All your models are! And, with differentiable programming, they become <em>trainable functions</em>.</p>
<h3 id="what-makes-it-tick">What Makes It Tick</h3>
<p>My overall thesis of this series is about four essential properties of executing effective differentiable programing-based models. All of these things, I feel, have to come together seamlessly to make this all work.</p>
<ol type="1">
<li><p><em>Functional programming</em>, allowing us to write higher-order functions and combinators that take functions and return functions.</p>
<p>This is the entire crux of this approach, and lets us not only draw from mathematical models directly, but also combine and reshape models in arbitrary ways just by using normal function composition and application, instead of being forced into a rigid compositional model.</p>
<p>We were able to chain, fork, recombine simple model primitives to make <em>new</em> models by just writing normal higher-order functions. In fact, as we will see in the upcoming posts, we can actually re-use higher order functions like <code>foldl</code> and <code>map</code> that are already commonly used in functional programming.</p>
<p>In the upcoming posts, we will take this principle to the extreme. We’ll define more combinators like <code>(&lt;~)</code> and see how many models we think are “fundamental” (like recurrent neural networks, autoregressive models) really are just combinators applied to even simpler models.</p>
<p>The role of these combinators is not <em>essential</em>, but <em>helpful</em> — we could always fall back on normal function composition, but higher-order functions and combinators let us encapsulate certain repeating design patterns and transformations.</p></li>
<li><p><em>Differentiable</em> programs, allowing us to write normal functions and have them be automatically differentiable for gradient descent.</p>
<p>I’m not sure at this point if this is best when supported at the language/compiler level, or at the library level. Whatever it is, though, the combination of differentiable programming with higher-order functions and other functional programming fundamentals is what makes this particularly powerful.</p></li>
<li><p><em>Purely</em> functional programming. If <em>any</em> of these functions were side-effecting and impure functions, the correspondence between functions and mathematical models completely falls apart. This is something we often take for granted when writing Haskell, but in other languages, without purity, no model is sound. If we are writing in a non-pure language, we have to consider this as an explicit assumption.</p></li>
<li><p>A <em>strong expressive static type system with type inference</em> makes this all reasonable to work with.</p>
<p>A lot of the combinators in this approach (like <code>(&lt;~)</code>) manipulate the <em>type</em> of model parameters, and if we use a lot of them, it becomes either impossible or unfeasible to manage it all in our heads. Without the help of a compiler, it would be impossible to sanely write complex programs. Having a statically type system with <em>type inference</em> allows the compiler to keep track of these for us and manage parameter shapes, and lets us ask questions about the parameters that our models have at compile-time.</p>
<p>For example, note how in our <code>twoLayer</code> definition, we left a type wildcard so the compiler can fill in the type for us.</p>
<p>We’ll also see in later posts that if we pick the types of our combinators correctly, the compiler can sometimes basically write our code for us.</p>
<p>In addition, having to think about types forces us to think, ahead of time, about how types interact. This thought process itself often yields important insight.</p></li>
</ol>
<p>In the <a href="https://blog.jle.im/entry/purely-functional-typed-models-2.html">next post</a>, we will explore how to reap the surprising benefits of this purely functional typed style when applying it to stateful and recurrent models.</p>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p>Those familiar with Haskell idioms might recognize this type as being essentially <code>a -&gt; Reader p b</code> (or <code>Kleisli (Reader p) a b</code>) which roughly represents the notion of “A function from <code>a</code> to <code>b</code> with an ‘environment’ of type <code>p</code>”.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Note that this is only sound as a loss function for a single “scalar value”, like <code>Double</code> or a one-vector. In general, we’d have this take a loss function as a parameter.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div><footer><hr><div class="copy-content"><p>Hi, thanks for reading! You can reach me via email at <a href="mailto:justin@jle.im" class="email">justin@jle.im</a>, or at twitter at <a href="https://twitter.com/mstk">@mstk</a>! This post and all others are published under the <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/">CC-BY-NC-ND 3.0</a> license. Corrections and edits via pull request are welcome and encouraged at <a href="https://github.com/mstksg/inCode">the source repository</a>.</p>
<p>If you feel inclined, or this post was particularly helpful for you, why not consider <a href="https://www.patreon.com/justinle/overview">supporting me on Patreon</a>, or a <a href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">BTC donation</a>? :)</p></div><div class="clear"></div><ul class="entry-series"><li><div>This entry is a part of a series called <b>&quot;Functional Models&quot;</b>.  Find the rest of the entries in this series at its <a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series" title="+Functional Models"> series history</a>.</div></li></ul><ul class="tag-list"><li><a href="https://blog.jle.im/entries/tagged/backprop.html" class="tag-a-tag">#backprop</a></li><li><a href="https://blog.jle.im/entries/tagged/deep-learning.html" class="tag-a-tag">#deep learning</a></li><li><a href="https://blog.jle.im/entries/tagged/differentiable-programming.html" class="tag-a-tag">#differentiable programming</a></li><li><a href="https://blog.jle.im/entries/tagged/machine-learning.html" class="tag-a-tag">#machine learning</a></li><li><a href="https://blog.jle.im/entries/tagged/modeling.html" class="tag-a-tag">#modeling</a></li><li><a href="https://blog.jle.im/entries/category/@haskell.html" class="tag-a-category">@HASKELL</a></li><li><a href="https://blog.jle.im/entries/series/+functional-models.html" class="tag-a-series">+Functional Models</a></li></ul><aside class="social-buttons"><div class="addthis_toolbox addthis_default_style addthis-buttons"><a class="addthis_button_facebook_like" fb:like:layout="button_count"></a><a class="addthis_button_tweet"></a><a class="addthis_button_google_plusone" g:plusone:size="medium"></a><a class="addthis_counter addthis_pill_style"></a></div><div class="custom-social-buttons"><div class="custom-social-button"><a href="https://www.reddit.com/submit" onclick="window.location = &#39;https://www.reddit.com/submit?url=&#39;+ encodeURIComponent(window.location); return false"><img src="https://www.reddit.com/static/spreddit7.gif" alt="submit to reddit"></a></div></div></aside><nav class="next-prev-links"><ul><li class="prev-entry-link">&larr; <a href="https://blog.jle.im/entry/const-applicative-and-monoids.html">The Const Applicative and Monoids</a> (Previous)</li><li class="next-entry-link">(Next) <a href="https://blog.jle.im/entry/purely-functional-typed-models-2.html">A Purely Functional Typed Approach to Trainable Models (Part 2)</a> &rarr;</li></ul></nav></footer></article><div class="post-entry"><div class="tile"><div id="disqus_thread"></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'https://blog.jle.im/entry/purely-functional-typed-models-1.html';
    this.page.identifier = 'functional-models-1';
};
(function() {
    var d = document, s = d.createElement('script');
    s.src = '//incode.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a><br></noscript><a href="http://disqus.com" class="dsq-brlink">Comments powered by <span class="logo-disqus">Disqus</span></a></div></div></div></div></div><div id="footer-container"><div id="footer-content"><div class="tile"><div class="footer-copyright">&copy; 2020 Justin Le <span class="license-link">(<a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" class="license">CC-BY-NC-ND 3.0</a>)</span></div><div class="footer-follow social-follows"><ul class="social-follows-list"><li><ul class="social-follows-list-social"><li><a class="social-follow-twitter" title="Follow me on Twitter!" href="https://twitter.com/intent/user?user_id=mstk" onclick="window.open(
  &#39;http://twitter.com/intent/user?user_id=907281&#39;,
  &#39;facebook-share-dialog&#39;,
  &#39;width=550,height=520&#39;);
return false;
">Twitter</a></li><li><a class="social-follow-github" title="Fork me on Github!" href="https://github.com/mstksg">Github</a></li><li><a class="social-follow-twitch" title="Watch me on Twitch!" href="https://www.twitch.tv/justin_l">Twitch</a></li><li><a class="social-follow-patreon" title="Support me on Patreon!" href="https://www.patreon.com/justinle/overview">Patreon</a></li><li><a class="social-follow-gplus" title="Add me on Google+!" href="https://plus.google.com/+JustinLe">Google+</a></li><li><a class="social-follow-keybase" title="Track me on Keybase!" href="https://keybase.io/mstksg">Keybase</a></li><li><a class="social-follow-linkedin" title="Connect with me on LinkedIn!" href="https://linkedin.com/in/lejustin">LinkedIn</a></li><li><a class="social-follow-bitcoin" title="Donate via bitcoin!" href="bitcoin:3D7rmAYgbDnp4gp4rf22THsGt74fNucPDU">Bitcoin</a></li></ul></li><li><ul class="social-follows-list-site"><li><a class="social-follow-rss" title="Subscribe to my RSS Feed!" href="http://feeds.feedburner.com/incodeblog">RSS</a></li><li><a class="social-follow-email" title="Subscribe to the mailing list!" href="https://feedburner.google.com/fb/a/mailverify?loc=en_US&amp;uri=incodeblog">Mailing list</a></li></ul></li></ul></div></div></div></div></body></html>